{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Translation Word/Char Count Prediction (Part 1)\"\n",
    "\n",
    "> Prediction of translated Word or Char Count is used as a Quality or Validation Check\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2\n",
    "- image: images/PredictTranslationWordAndCharCount_1.png\n",
    "- categories: [Translation Industry,  Regression,  Python,pandas]\n",
    "- show_tags: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klGNgWREsvQv"
   },
   "source": [
    "# \"TF-Agents-CartPole\"\n",
    "\n",
    "> Reinforcement Learning (RL) to control the balancing of a pole on a moving cart\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2\n",
    "- image: images/graphical_representation_of_rl.png\n",
    "- categories: [Control,   RL,   TensorFlow,TF-Agents,Python]\n",
    "- show_tags: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4E8ujfcND6pJ",
    "outputId": "a8da74d2-621c-43dd-ced7-971cb31d5901",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)\n",
    "# root_dir = \"/content/gdrive/My Drive/\"\n",
    "# base_dir = root_dir + 'RL/TF-Agents/blog_posts/TF-Agents-CartPole/'\n",
    "# # base_dir = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsaQlK8fFQqH"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The cart-pole problem can be considered as the \"Hello World\" problem of Reinforcement Learning (RL). It was described by [Barto (1983)](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf). The physics of the system is as follows:\n",
    "\n",
    "* All motion happens in a vertical plane\n",
    "* A hinged pole is attached to a cart\n",
    "* The cart slides horizontally on a track in an effort to balance the pole vertically\n",
    "* The system has four state variables:\n",
    "\n",
    "$x$: displacement of the cart\n",
    "\n",
    "$\\theta$: vertical angle on the pole\n",
    "\n",
    "$\\dot{x}$: velocity of the cart\n",
    "\n",
    "$\\dot{\\theta}$: angular velocity of the pole\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0-ZNxUXoMR6"
   },
   "source": [
    "## 2. Purpose\n",
    "\n",
    "The purpose of our activity in this blog post is to construct and train an entity, let's call it a *controller*, that can manage the horizontal motions of the cart so that the pole remains as close to vertical as possible. The controlled entity is, of course, the *cart and pole* system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1u9QVVsShC9X"
   },
   "source": [
    "## 3. TF-Agents Setup\n",
    "\n",
    "We will use the Tensorflow TF-Agents framework. In addition, this notebook will need to run in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEHR2Ui-lo8O",
    "outputId": "3f72ecfb-fd77-480b-98c3-fed33a6a6818"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
      "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
      "Requirement already satisfied: imageio==2.4.0 in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio==2.4.0) (7.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio==2.4.0) (1.19.5)\n",
      "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (2.0)\n",
      "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
      "Requirement already satisfied: tf-agents in /usr/local/lib/python3.6/dist-packages (0.7.1)\n",
      "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.17.3)\n",
      "Requirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (7.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (3.7.4.3)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (3.12.4)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.19.5)\n",
      "Requirement already satisfied: tensorflow-probability>=0.12.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.12.1)\n",
      "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.10.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.15.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.17.0->tf-agents) (1.4.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.17.0->tf-agents) (1.5.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.11.3->tf-agents) (51.1.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (4.4.2)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.1.5)\n",
      "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.3.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install -y xvfb ffmpeg\n",
    "!pip install 'imageio==2.4.0'\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install tf-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sMitx5qSgJk1"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "1K7_uLc_nrCs",
    "outputId": "5c9b43e3-538c-4e5e-c479-64d586bc7801"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm53yvG3naxc"
   },
   "source": [
    "The following is needed for rendering a virtual display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "J6HsdS5GbSjd"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2WiLAoSwA_KI"
   },
   "outputs": [],
   "source": [
    "# ![Figure 1 Graphical Representation](../images/graphical_representation_of_rl.png)\n",
    "# ![Figure 1 Graphical Representation](/content/gdrive/My Drive/RL/TF-Agents/blog_posts/TF-Agents-CartPole/graphical_representation_of_rl.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FGSsxkOkEkxF",
    "outputId": "feb1af4d-c388-4fa8-9b9d-fcc28f594ed0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/gdrive/My Drive/RL/TF-Agents/blog_posts/TF-Agents-CartPole/'"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1s77CKrv1m_"
   },
   "source": [
    "## 4. Hyperparameters\n",
    "Here we specify all the hyperparameters for the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "psVAtI4Vv4zL"
   },
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 20000\n",
    "\n",
    "INITIAL_COLLECT_STEPS = 100\n",
    "COLLECT_STEPS_PER_ITERATION = 1\n",
    "REPLAY_BUFFER_MAX_LENGTH = 100000\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "LOG_INTERVAL = 200\n",
    "\n",
    "NUM_EVAL_EPISODES = 10\n",
    "EVAL_INTERVAL = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w913YTn-4pOU"
   },
   "source": [
    "## 5. Graphical Representation of the Problem\n",
    "\n",
    "We will work with a graphical representation of our cart-and-pole problem, rather than to just ramble on with words. This will enhance the description. The graphic will also include some TF-Agents specifics. Here is the representation:\n",
    "\n",
    "![Figure 1 Graphical Representation](./graphical_representation_of_rl.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KRG0AYE6Ocgy"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "# import Image\n",
    "# im = PIL.Image.open(base_dir+'graphical_representation_of_rl.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMsJC3DEgI0x"
   },
   "source": [
    "## 6. Environment\n",
    "\n",
    "Let's start with the controller. In Reinforcement Learning, the controlled entity is known as an **environment**. The TF-Agents framework contain some ready to use environments that can be created in TF-Agents using the `tf_agents.environments` suites. Fortunately, it makes access to the cart-and-pole environment (setup by OpenAI Gym) easy. Next, we load the cart-and-pole environment from the OpenAI Gym suite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pYEz-S9gEv2-"
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIHYVBkuvPNw"
   },
   "source": [
    "You can render this environment to see how it looks. A free-swinging pole is attached to a cart.  The goal is to move the cart right or left in order to keep the pole pointing up. To verify, we can inspect our loaded environment with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSmAGfjnRzUE"
   },
   "source": [
    "### Input to Environment\n",
    "\n",
    "The specification of inputs to the environment is provided by the `env.action_spec` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBBaR-jTSJ3B",
    "outputId": "b3e5bc83-62ce-4872-9fe1-45345af85eeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsQ8Qb5gSXSP"
   },
   "source": [
    "`shape` specifies the structure of the input which is a scalar in this case. `dtype` is the data type which is an `int64`. The `minimum` value of the action is `0` and the `maximum` is `1`. We will use the convention that the `action` on the cart is as follows:\n",
    "\n",
    "* `0` means LEFT\n",
    "* `1` means RIGHT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_fatIpdUMMg"
   },
   "source": [
    "### Evolution of the Environment\n",
    "\n",
    "The arrival of an `action` at the input of the environment leads to the update of its state. This is how the environment evolves. To advance the state of the environment, the `environment.step` method takes an input `action` and returns a `TimeStep` tuple containing the next observation of the environment and the reward for the action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MoIFnVtVJuA"
   },
   "source": [
    "### Output from Environment\n",
    "\n",
    "The specification of output from the environment is provided by the `env.time_step_spec` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxRXy0PPSJzL",
    "outputId": "4481372e-df1d-45ba-cd56-6409be17dddd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'), reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), discount=BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]))"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.time_step_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtWKdiFKVouy"
   },
   "source": [
    "This specification has the following fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LyEA8FFHSJvj",
    "outputId": "988e4742-8176-4bab-e05c-bdda07368ad8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type', 'reward', 'discount', 'observation')"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.time_step_spec()._fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4cg3P9wcmOb"
   },
   "source": [
    "The `step_type` indicates whether a step is the first step, a middle step, or the last step in an episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qvXKTwO7SJks",
    "outputId": "3b87670c-5c42-4943-a656-b620b3fe3a50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.time_step_spec().step_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byT1cNWOc-de"
   },
   "source": [
    "The `reward` is a scalar which conveys the reward from the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mbWtFTCRdBou",
    "outputId": "d1e8d3eb-80d6-4238-9826-ecac556dfa8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.time_step_spec().reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb3PnaHMdVSH"
   },
   "source": [
    "The `discount` is a factor that modifies the `reward`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEvlLMBSdBhp",
    "outputId": "842647b0-1b09-4c32-903f-01d78afd1730"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.time_step_spec().discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnMNcvGwdluI"
   },
   "source": [
    "The `observation` is the observable state of the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4ImxIaRdBc0",
    "outputId": "dc41b927-2ac2-4486-f8ef-e93f238b6fb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.time_step_spec().observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zN7rw5a-d6v3"
   },
   "source": [
    "In this case we have a vector with 4 elements - one each for the cart displacement, cart velocity, pole angle, and pole angular velocity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVllomMLfPzO"
   },
   "source": [
    "### Demonstrate the evolution of the environment\n",
    "\n",
    "Let's submit 10 `RIGHT` actions to the environment, just for fun:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dye0LOUvgr5T"
   },
   "source": [
    "It is interesting to see an agent actually performing a task in an environment. \n",
    "\n",
    "First, create a function to embed videos in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "l7YqTromgr5T"
   },
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "  return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-D4Z-7X9gr5U"
   },
   "source": [
    "Now iterate through a few episodes of the Cartpole game with the agent. The underlying Python environment (the one \"inside\" the TensorFlow environment wrapper) provides a `render()` method, which outputs an image of the environment state. These can be collected into a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wLQGQZVHgr5U"
   },
   "outputs": [],
   "source": [
    "def create_video(filename, action, num_steps=10, fps=30):\n",
    "  filename = filename + \".mp4\"\n",
    "  env.reset()\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    video.append_data(env.render())\n",
    "    for _ in range(num_steps):\n",
    "      tstep = env.step(action); print(tstep)\n",
    "      video.append_data(env.render())\n",
    "  return embed_mp4(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fgarXWXydBaF"
   },
   "outputs": [],
   "source": [
    "action = np.array(1, dtype=np.int32) #move RIGHT action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "MSjp-UcydBXL"
   },
   "outputs": [],
   "source": [
    "# create_video(\"untrained-agent\", action, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JSc9GviWUBK"
   },
   "source": [
    "We will use two environments: one for training and one for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "N7brXNIGWXjC"
   },
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoXsA9sarHO7"
   },
   "source": [
    "### Convert environments to TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuUqXAVmecTU"
   },
   "source": [
    "The Cartpole environment, like most environments, is written in pure Python. This is converted to TensorFlow using the `TFPyEnvironment` wrapper.\n",
    "\n",
    "The original environment's API uses Numpy arrays. The `TFPyEnvironment` converts these to `Tensors` to make it compatible with Tensorflow agents and policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Xp-Y4mD6eDhF"
   },
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9lW_OZYFR8A"
   },
   "source": [
    "## 7. Agent\n",
    "\n",
    "The controller in our problem is the algorithm used to solve the problem. In RL parlance the controller is known as an `Agent`. TF-Agents provides standard implementations of a variety of `Agents`, including:\n",
    "\n",
    "-   [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
    "-   [REINFORCE](https://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n",
    "-   [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n",
    "-   [TD3](https://arxiv.org/pdf/1802.09477.pdf)\n",
    "-   [PPO](https://arxiv.org/abs/1707.06347)\n",
    "-   [SAC](https://arxiv.org/abs/1801.01290).\n",
    "\n",
    "For our problem we will use the DQN agent. The DQN agent can be used in any environment which has a discrete action space.\n",
    "\n",
    "The fundamental problem for an Agent is how to find the next best action to submit to the environment. In the case of a DQN Agent the agent makes use of a `QNetwork`, which is a neural network model that can learn to predict `QValues` (expected returns) for all actions, given an observation from the environment. By inspecting the `QValues`, the agent can decide on the best next action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWBr2y4_tYIw"
   },
   "source": [
    "### QNetwork\n",
    "\n",
    "We use `tf_agents.networks.q_network` to create a `QNetwork`, passing in the `observation_spec`, `action_spec`, and a tuple `fc_layer_params` describing the number and size of the model's hidden layers. Each value in the tuple specifies the number of neurons for that hidden layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "TgkdEPg_muzV"
   },
   "outputs": [],
   "source": [
    "fc_layer_params = (100,)\n",
    "q_net = q_network.QNetwork(\n",
    "    input_tensor_spec= train_env.observation_spec(), \n",
    "    action_spec=       train_env.action_spec(), \n",
    "    fc_layer_params=   fc_layer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z62u55hSmviJ"
   },
   "source": [
    "### DqnAgent\n",
    "We now use `tf_agents.agents.dqn.dqn_agent` to instantiate a `DqnAgent`. In addition to the `time_step_spec`, `action_spec` and the QNetwork, the agent constructor also requires an optimizer (in this case, `AdamOptimizer`), a loss function, and an integer step counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "jbY4yrjTEyc9"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "train_step_counter = tf.Variable(0)\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    time_step_spec=     train_env.time_step_spec(),\n",
    "    action_spec=        train_env.action_spec(),\n",
    "    q_network=          q_net,\n",
    "    optimizer=          optimizer,\n",
    "    td_errors_loss_fn=  common.element_wise_squared_loss,\n",
    "    train_step_counter= train_step_counter)\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0KLrEPwkn5x"
   },
   "source": [
    "### Policies\n",
    "\n",
    "A policy defines the way an agent acts relative to the environment. The goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome.\n",
    "\n",
    "In this problem:\n",
    "\n",
    "-   The desired outcome is keeping the pole balanced vertically over the cart\n",
    "-   The policy returns an action (LEFT or RIGHT) for each `TimeStep`'s `observation`\n",
    "\n",
    "Agents contain two policies: \n",
    "\n",
    "-   `agent.policy` — The main policy that is used for evaluation and deployment.\n",
    "-   `agent.collect_policy` — A second policy that is used for data collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BwY7StuMkuV4",
    "outputId": "9d4f0f53-1b01-4290-daa7-6c35345e0f27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.greedy_policy.GreedyPolicy at 0x7f1cdb320b00>"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_policy = agent.policy\n",
    "eval_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aDRtP8G-yxEd",
    "outputId": "3125fb35-c846-469d-b7cf-6b2dd79eee0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy at 0x7f1cdb3209e8>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_policy = agent.collect_policy\n",
    "collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Qs1Fl3dV0ae"
   },
   "source": [
    "Policies can be created independently of agents. For example, use `tf_agents.policies.random_tf_policy` to create a policy which will randomly select an action for each `time_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "HE37-UCIrE69"
   },
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "    time_step_spec= train_env.time_step_spec(), \n",
    "    action_spec=    train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOlnlRRsUbxP"
   },
   "source": [
    "To get an action from a policy, call the `policy.action(tstep)` method. The `tstep` of type `TimeStep` contains the observation from the environment. This method returns a `PolicyStep`, which is a named tuple with three components:\n",
    "\n",
    "-   `action` — the action to be taken (in this case, `0` or `1`)\n",
    "-   `state` — used for stateful (that is, RNN-based) policies\n",
    "-   `info` — auxiliary data, such as log probabilities of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sLfx_CzzNz8"
   },
   "source": [
    "Let's create an example environment and setup a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "5gCcpXswVAxk"
   },
   "outputs": [],
   "source": [
    "example_environment = tf_py_environment.TFPyEnvironment(\n",
    "    suite_gym.load('CartPole-v0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZgkSaGhzvgk"
   },
   "source": [
    "We reset this environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D4DHZtq3Ndis",
    "outputId": "2801d73c-ef50-40a4-f08e-2c7d59ab0651"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
       "array([[ 0.04777068, -0.02450945, -0.03095539, -0.01279974]],\n",
       "      dtype=float32)>)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tstep = example_environment.reset()\n",
    "tstep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zqQMzy9Pz4Py",
    "outputId": "defb56df-1d50-4d5f-bfeb-3d8dcbe78600"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type', 'reward', 'discount', 'observation')"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tstep._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rt37ROxD0FXn",
    "outputId": "e2f1fd23-1624-4e8b-ede2-966d9c43fee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([1.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([[ 0.04777068 -0.02450945 -0.03095539 -0.01279974]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tstep.step_type)\n",
    "print(tstep.reward)\n",
    "print(tstep.discount)\n",
    "print(tstep.observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWSFDcAt0X4c"
   },
   "source": [
    "Now we find the `PolicyStep` from which the next `action` can be found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRFqAUzpNaAW",
    "outputId": "c4ed3612-6e10-4953-d13a-5fda65424186"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=())"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pstep = random_policy.action(tstep)\n",
    "pstep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7llvqyLyfAT",
    "outputId": "5b1ed0a5-da79-4508-f7c5-28acc4b9a01e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('action', 'state', 'info')"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pstep._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJHQbbb586DK",
    "outputId": "b48eb33e-827f-4faa-b5ef-0177dcf5fbc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "()\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(pstep.action)\n",
    "print(pstep.state)\n",
    "print(pstep.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94rCXQtbUbXv"
   },
   "source": [
    "## 8. Metrics and Evaluation\n",
    "\n",
    "The most common metric used to evaluate a policy is the **average return**. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
    "\n",
    "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "bitzHo5_UbXy"
   },
   "outputs": [],
   "source": [
    "def compute_avg_return(env, pol, num_episodes=10):\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "    tstep = env.reset()\n",
    "    episode_return = 0.0\n",
    "    while not tstep.is_last():\n",
    "      pstep = pol.action(tstep)\n",
    "      tstep = env.step(pstep.action)\n",
    "      episode_return += tstep.reward\n",
    "    total_return += episode_return\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_snCVvq5Z8lJ"
   },
   "source": [
    "Running this computation on the `random_policy` shows a baseline performance in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKFMXrtGzdqy",
    "outputId": "54cd979f-d776-4955-c779-82655dd80c5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EVAL_EPISODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bgU6Q6BZ8Bp",
    "outputId": "c198a665-31ca-4686-b01d-fbe40f83b58f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.4"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, NUM_EVAL_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLva6g2jdWgr"
   },
   "source": [
    "## 9. Replay Buffer\n",
    "\n",
    "The replay buffer keeps track of data collected from the environment. We will use `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n",
    "\n",
    "The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "vX2zGUWJGWAl"
   },
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=  agent.collect_data_spec,\n",
    "    batch_size= train_env.batch_size,\n",
    "    max_length= REPLAY_BUFFER_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGNTDJpZs4NN"
   },
   "source": [
    "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_IZ-3HcqgE1z",
    "outputId": "08b23655-717f-4451-dd36-4f4a7469aee1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "      dtype=float32)), action=BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sy6g1tGcfRlw",
    "outputId": "28b3d37c-beed-4161-f860-d7a444dafb0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVD5nQ9ZGo8_"
   },
   "source": [
    "## 10. Data Collection\n",
    "\n",
    "Now we execute the random policy in the environment for a few steps, recording the data in the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "wr1KSAEGG4h9"
   },
   "outputs": [],
   "source": [
    "def collect_step(env, pol, buffer):\n",
    "  tstep = env.current_time_step()\n",
    "  pstep = pol.action(tstep)\n",
    "  next_tstep = env.step(pstep.action)\n",
    "  traj = trajectory.from_transition(tstep, pstep, next_tstep)\n",
    "  buffer.add_batch(traj) # Add trajectory to the replay buffer\n",
    "\n",
    "def collect_data(env, pol, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, pol, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, INITIAL_COLLECT_STEPS)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations. \n",
    "# For more details see the drivers module.\n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84z5pQJdoKxo"
   },
   "source": [
    "The replay buffer is now a collection of Trajectories. Let's inspect one of the Trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wZnLu2ViO4E",
    "outputId": "2a9717b0-7876-4d92-ec7e-5f5ab76ad9e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "2\n",
      "(Trajectory(step_type=<tf.Tensor: shape=(), dtype=int32, numpy=1>, observation=<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.01126854, -0.16798736,  0.10049265,  0.40535218], dtype=float32)>, action=<tf.Tensor: shape=(), dtype=int64, numpy=0>, policy_info=(), next_step_type=<tf.Tensor: shape=(), dtype=int32, numpy=1>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, discount=<tf.Tensor: shape=(), dtype=float32, numpy=1.0>), BufferInfo(ids=<tf.Tensor: shape=(), dtype=int64, numpy=34>, probabilities=<tf.Tensor: shape=(), dtype=float32, numpy=0.01>))\n"
     ]
    }
   ],
   "source": [
    "traj = iter(replay_buffer.as_dataset()).next()\n",
    "print(type(traj))\n",
    "print(len(traj))\n",
    "print(traj);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPlmzAvLWwO1",
    "outputId": "056c0d4d-8b54-4b9a-ce68-e47b099896a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(step_type=<tf.Tensor: shape=(), dtype=int32, numpy=1>, observation=<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.01126854, -0.16798736,  0.10049265,  0.40535218], dtype=float32)>, action=<tf.Tensor: shape=(), dtype=int64, numpy=0>, policy_info=(), next_step_type=<tf.Tensor: shape=(), dtype=int32, numpy=1>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, discount=<tf.Tensor: shape=(), dtype=float32, numpy=1.0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d-uvmakNX623",
    "outputId": "abd2091e-8f29-42e8-cae2-a79b2787c08e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf_agents.trajectories.trajectory.Trajectory"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(traj[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-jFiMstgYHuK",
    "outputId": "364684d1-d3ed-48da-f219-c671168c10e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj[0]._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0xF1tnC6msn",
    "outputId": "5df3910b-1f79-45a8-81c4-0445e8e845ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_type: tf.Tensor(1, shape=(), dtype=int32)\n",
      "observation: tf.Tensor([-0.01126854 -0.16798736  0.10049265  0.40535218], shape=(4,), dtype=float32)\n",
      "action: tf.Tensor(0, shape=(), dtype=int64)\n",
      "policy_info: ()\n",
      "next_step_type: tf.Tensor(1, shape=(), dtype=int32)\n",
      "reward: tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "discount: tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('step_type:', traj[0].step_type)\n",
    "print('observation:', traj[0].observation)\n",
    "print('action:', traj[0].action)\n",
    "print('policy_info:', traj[0].policy_info)\n",
    "print('next_step_type:', traj[0].next_step_type)\n",
    "print('reward:', traj[0].reward)\n",
    "print('discount:', traj[0].discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w2J7SAu-YfZ0",
    "outputId": "968f468e-8ba2-46e1-921c-380eabe87355"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BufferInfo(ids=<tf.Tensor: shape=(), dtype=int64, numpy=34>, probabilities=<tf.Tensor: shape=(), dtype=float32, numpy=0.01>)"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0qQL2QhUYfRk",
    "outputId": "0ba3e801-1afa-48f9-8b36-5c67858b9815"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf_agents.replay_buffers.tf_uniform_replay_buffer.BufferInfo"
      ]
     },
     "execution_count": 88,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(traj[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fwgcbf8Y2p_",
    "outputId": "1bc82608-c066-4c67-efc0-30c2a29e976b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ids', 'probabilities')"
      ]
     },
     "execution_count": 89,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj[1]._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6qg48cmY2mF",
    "outputId": "42be86b1-dc1b-4950-c1cb-49ba025f3d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: tf.Tensor(34, shape=(), dtype=int64)\n",
      "probabilities: tf.Tensor(0.01, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('ids:', traj[1].ids)\n",
    "print('probabilities:', traj[1].probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TujU-PMUsKjS"
   },
   "source": [
    "The agent needs access to the replay buffer. TF-Agents provide this access by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
    "\n",
    "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
    "\n",
    "The code also optimize this dataset by running parallel calls and prefetching data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QWuA3GbfiASI",
    "outputId": "c5d46e77-1f5e-48c5-d018-0509514cd4f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba7bilizt_qW",
    "outputId": "47a337e5-1787-4750-e245-0b983da7aef0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(step_type=(64, 2), observation=(64, 2, 4), action=(64, 2), policy_info=(), next_step_type=(64, 2), reward=(64, 2), discount=(64, 2)), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(step_type=tf.int32, observation=tf.float32, action=tf.int64, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 92,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=BATCH_SIZE, \n",
    "    num_steps=2).prefetch(3)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K13AST-2ppOq",
    "outputId": "93c9c140-a0e6-4247-d494-5e4514c38a73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f1cd88b10f0>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "Th5w5Sff0b16"
   },
   "outputs": [],
   "source": [
    "# Compare this representation of replay data \n",
    "# to the collection of individual trajectories shown earlier:\n",
    "# iterator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBc9lj9VWWtZ"
   },
   "source": [
    "## 11. Training the agent\n",
    "\n",
    "Two things must happen during the training loop:\n",
    "\n",
    "-   collect data from the environment\n",
    "-   use that data to train the agent's neural network(s)\n",
    "\n",
    "This example also periodicially evaluates the policy and prints the current score.\n",
    "\n",
    "The following will take ~5 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJ-Bvvlf7uIN"
   },
   "outputs": [],
   "source": [
    "# num_iterations = 20000 # @param {type:\"integer\"} #.\n",
    "\n",
    "# initial_collect_steps = 100  # @param {type:\"integer\"} \n",
    "# collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "# replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "# batch_size = 64  # @param {type:\"integer\"}\n",
    "# learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "# log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "# num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "# eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "8vdorhCI74h2"
   },
   "outputs": [],
   "source": [
    "# NUM_ITERATIONS\n",
    "NUM_ITERATIONS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pTbJ3PeyF-u",
    "outputId": "ea229e20-17e2-4e3f-a475-a9d9c8f42904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.96 µs\n",
      "step = 200: loss = 195.16213989257812\n",
      "step = 400: loss = 226.61244201660156\n",
      "step = 600: loss = 274.764892578125\n",
      "step = 800: loss = 42.2076416015625\n",
      "step = 1000: loss = 6.291749954223633\n",
      "step = 1000: Average Return = 33.79999923706055\n",
      "step = 1200: loss = 50.53717803955078\n",
      "step = 1400: loss = 36.393985748291016\n",
      "step = 1600: loss = 172.8566436767578\n",
      "step = 1800: loss = 11.371061325073242\n",
      "step = 2000: loss = 18.2037353515625\n",
      "step = 2000: Average Return = 72.80000305175781\n",
      "step = 2200: loss = 54.74932861328125\n",
      "step = 2400: loss = 16.711841583251953\n",
      "step = 2600: loss = 3.500059127807617\n",
      "step = 2800: loss = 7.880290985107422\n",
      "step = 3000: loss = 8.416237831115723\n",
      "step = 3000: Average Return = 95.5999984741211\n",
      "step = 3200: loss = 607.7373657226562\n",
      "step = 3400: loss = 5.996950149536133\n",
      "step = 3600: loss = 54.01152801513672\n",
      "step = 3800: loss = 238.8196258544922\n",
      "step = 4000: loss = 21.115650177001953\n",
      "step = 4000: Average Return = 71.80000305175781\n",
      "step = 4200: loss = 247.99981689453125\n",
      "step = 4400: loss = 326.7658996582031\n",
      "step = 4600: loss = 8.009516716003418\n",
      "step = 4800: loss = 228.2291717529297\n",
      "step = 5000: loss = 10.485452651977539\n",
      "step = 5000: Average Return = 175.89999389648438\n",
      "step = 5200: loss = 217.0672607421875\n",
      "step = 5400: loss = 200.13392639160156\n",
      "step = 5600: loss = 202.6009521484375\n",
      "step = 5800: loss = 5.376280784606934\n",
      "step = 6000: loss = 5.735952377319336\n",
      "step = 6000: Average Return = 198.10000610351562\n",
      "step = 6200: loss = 54.103065490722656\n",
      "step = 6400: loss = 89.71216583251953\n",
      "step = 6600: loss = 77.9834213256836\n",
      "step = 6800: loss = 276.6645812988281\n",
      "step = 7000: loss = 261.430419921875\n",
      "step = 7000: Average Return = 195.0\n",
      "step = 7200: loss = 92.05532836914062\n",
      "step = 7400: loss = 13.44973373413086\n",
      "step = 7600: loss = 31.970050811767578\n",
      "step = 7800: loss = 18.705116271972656\n",
      "step = 8000: loss = 148.3210906982422\n",
      "step = 8000: Average Return = 198.60000610351562\n",
      "step = 8200: loss = 12.089221954345703\n",
      "step = 8400: loss = 275.4737854003906\n",
      "step = 8600: loss = 461.70880126953125\n",
      "step = 8800: loss = 740.7882690429688\n",
      "step = 9000: loss = 15.471342086791992\n",
      "step = 9000: Average Return = 200.0\n",
      "step = 9200: loss = 13.399641990661621\n",
      "step = 9400: loss = 23.77669906616211\n",
      "step = 9600: loss = 16.250200271606445\n",
      "step = 9800: loss = 18.584796905517578\n",
      "step = 10000: loss = 15.69692325592041\n",
      "step = 10000: Average Return = 200.0\n",
      "step = 10200: loss = 746.649169921875\n",
      "step = 10400: loss = 573.50390625\n",
      "step = 10600: loss = 28.51150131225586\n",
      "step = 10800: loss = 793.2840576171875\n",
      "step = 11000: loss = 26.583303451538086\n",
      "step = 11000: Average Return = 200.0\n",
      "step = 11200: loss = 878.364501953125\n",
      "step = 11400: loss = 25.9678955078125\n",
      "step = 11600: loss = 1178.5902099609375\n",
      "step = 11800: loss = 652.8362426757812\n",
      "step = 12000: loss = 660.1619262695312\n",
      "step = 12000: Average Return = 200.0\n",
      "step = 12200: loss = 35.75788497924805\n",
      "step = 12400: loss = 665.1995239257812\n",
      "step = 12600: loss = 21.658220291137695\n",
      "step = 12800: loss = 34.46709060668945\n",
      "step = 13000: loss = 1340.4808349609375\n",
      "step = 13000: Average Return = 200.0\n",
      "step = 13200: loss = 2181.42626953125\n",
      "step = 13400: loss = 37.67461395263672\n",
      "step = 13600: loss = 3320.109130859375\n",
      "step = 13800: loss = 3381.054443359375\n",
      "step = 14000: loss = 670.6203002929688\n",
      "step = 14000: Average Return = 200.0\n",
      "step = 14200: loss = 43.05519485473633\n",
      "step = 14400: loss = 54.9406852722168\n",
      "step = 14600: loss = 43.494712829589844\n",
      "step = 14800: loss = 1596.6024169921875\n",
      "step = 15000: loss = 34.50205993652344\n",
      "step = 15000: Average Return = 200.0\n",
      "step = 15200: loss = 48.124488830566406\n",
      "step = 15400: loss = 61.94905471801758\n",
      "step = 15600: loss = 80.75376892089844\n",
      "step = 15800: loss = 57.689796447753906\n",
      "step = 16000: loss = 4747.44921875\n",
      "step = 16000: Average Return = 200.0\n",
      "step = 16200: loss = 3647.16845703125\n",
      "step = 16400: loss = 2504.968994140625\n",
      "step = 16600: loss = 58.82946014404297\n",
      "step = 16800: loss = 93.2029800415039\n",
      "step = 17000: loss = 3273.479248046875\n",
      "step = 17000: Average Return = 200.0\n",
      "step = 17200: loss = 2719.405517578125\n",
      "step = 17400: loss = 109.44195556640625\n",
      "step = 17600: loss = 267.1567687988281\n",
      "step = 17800: loss = 120.11451721191406\n",
      "step = 18000: loss = 3119.292724609375\n",
      "step = 18000: Average Return = 200.0\n",
      "step = 18200: loss = 62.79007339477539\n",
      "step = 18400: loss = 1214.75830078125\n",
      "step = 18600: loss = 73.04971313476562\n",
      "step = 18800: loss = 141.36680603027344\n",
      "step = 19000: loss = 112.62451934814453\n",
      "step = 19000: Average Return = 200.0\n",
      "step = 19200: loss = 7514.24609375\n",
      "step = 19400: loss = 7860.0087890625\n",
      "step = 19600: loss = 65.12687683105469\n",
      "step = 19800: loss = 3710.064697265625\n",
      "step = 20000: loss = 3101.4462890625\n",
      "step = 20000: Average Return = 200.0\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, NUM_EVAL_EPISODES)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(NUM_ITERATIONS):\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, COLLECT_STEPS_PER_ITERATION)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % LOG_INTERVAL == 0:\n",
    "    print(f'step = {step}: loss = {train_loss}')\n",
    "\n",
    "  if step % EVAL_INTERVAL == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, NUM_EVAL_EPISODES)\n",
    "    print(f'step = {step}: Average Return = {avg_return}')\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68jNcA_TiJDq"
   },
   "source": [
    "## Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO-LWCdbbOIC"
   },
   "source": [
    "### Plots\n",
    "\n",
    "Use `matplotlib.pyplot` to chart how the policy improved during training.\n",
    "\n",
    "One iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "NxtL1mbOYCVO",
    "outputId": "609b6010-8b34-43e2-ded8-98e850c3ef28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.725, 250.0)"
      ]
     },
     "execution_count": 109,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "iterations = range(0, NUM_ITERATIONS + 1, EVAL_INTERVAL)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7-XpPP99Cy7"
   },
   "source": [
    "### Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pGfGxSH32gn"
   },
   "source": [
    "Charts are nice. But more exciting is seeing an agent actually performing a task in an environment. \n",
    "\n",
    "First, create a function to embed videos in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "ULaGr8pvOKbl"
   },
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "  return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c_PH-pX4Pr5"
   },
   "source": [
    "Now iterate through a few episodes of the Cartpole game with the agent. The underlying Python environment (the one \"inside\" the TensorFlow environment wrapper) provides a `render()` method, which outputs an image of the environment state. These can be collected into a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "owOVWB158NlF"
   },
   "outputs": [],
   "source": [
    "# def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
    "def create_policy_eval_video(policy, filename, num_episodes=3, fps=30):\n",
    "  filename = filename + \".mp4\"\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = eval_env.reset()\n",
    "      video.append_data(eval_py_env.render())\n",
    "      while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        video.append_data(eval_py_env.render())\n",
    "  return embed_mp4(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "gf7rBUYt8lXT",
    "outputId": "1e61e4f9-5375-4b47-c02c-06894c94935d"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "# create_policy_eval_video(agent.policy, \"trained-agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "povaAOcZygLw"
   },
   "source": [
    "For fun, compare the trained agent (above) to an agent moving randomly. (It does not do as well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "pJZIdC37yNH4",
    "outputId": "0e0ab706-9e1c-40d4-e818-54e971527f67"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "# create_policy_eval_video(random_policy, \"random-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EM6S09fN8_uR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TF-Agents-CartPole.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
